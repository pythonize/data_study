{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3befb468",
   "metadata": {},
   "source": [
    "Wanted Crawling(이상현) _ 3팀 PFlow Project(No.3)\n",
    "\n",
    "\n",
    "Role\n",
    "\n",
    "- Wanted 데이터 수집\n",
    "- Wanted 웹크롤링, 데이터 프레임 형성, 파일 생성, SQL 적재 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b081b157",
   "metadata": {},
   "source": [
    "# 셀레니움 설치\n",
    "!pip install selenium\n",
    "# 패키지 설치\n",
    "!pip install webdriver_manager\n",
    "# 스텔스 기능 \n",
    "!pip install selenium-stealth\n",
    "# 시간 체크 기능\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def54e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일반적인 크롤링\n",
    "# Chrome 웹 드라이버 다운로드 및 설치 후 실행\n",
    "# 설치한 selenium에서 webdriver를 import\n",
    "# 크롤링 구문에서 사용할 필요 라이브러리 import\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium_stealth import stealth # 스텔스 기능\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm  # 상태바 모듈 불러오기\n",
    "import random  # random 모듈\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d5d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenum의 webdriver에 앞서 설치한 chromedirver를 연동\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d055807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 접속 사이트 주소 인식\n",
    "# `driver.get()` 내에 URL 주소를 입력하면 해당 주소로 이동\n",
    "url = 'https://www.wanted.co.kr/wdlist/518?country=kr&job_sort=job.recommend_order&years=-1&locations=all'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff7d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스크롤을 가장 아래로 내리는 함수\n",
    "def scroll_to_bottom(driver):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # 페이지의 가장 아래로 스크롤\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        # 새로운 내용이 로드될 때까지 대기\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # 새로운 페이지 높이 측정\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # 새로운 콘텐츠가 없으면 종료\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "# 스크롤 함수 호출\n",
    "scroll_to_bottom(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64eb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 가능한 페이지 길이 확인\n",
    "driver.execute_script('return document.body.scrollHeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90df162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup으로 파싱\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "req = driver.page_source\n",
    "soup = BeautifulSoup(req, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a821ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 페이지에 몇개 공고문 있는지 확인\n",
    "content = soup.select('.Card_Card__WdaEk')\n",
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93afcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 타이틀 문자열 확인\n",
    "content[1].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234193eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup으로 파싱\n",
    "req = driver.page_source\n",
    "soup = BeautifulSoup(req, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ce39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 타이틀 불러오기\n",
    "soup.find_all('p', class_ = 'Typography_Typography__root__RdAI1 Typography_Typography__body2__5Mmhi Typography_Typography__weightBold__KkJEY JobCard_JobCard__body__position__CyaGY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a922f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 화면 최상단으로 이동\n",
    "driver.execute_script('window.scrollTo(0, 0);')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e49d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_base = \"https://www.wanted.co.kr/\"\n",
    "# url_sub = \"wdlist/518?country=kr&job_sort=job.recommend_order&years=-1&selected=1025&selected=1024&selected=655&selected=1634&selected=669&locations=all\"\n",
    "# url = url_base + url_sub\n",
    "req = driver.page_source\n",
    "soup = BeautifulSoup(req, 'html.parser')\n",
    "soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9473e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공고문 제목만 불러오기\n",
    "title = soup.find_all(\"p\", class_ = \"Typography_Typography__root__RdAI1 Typography_Typography__body2__5Mmhi Typography_Typography__weightBold__KkJEY JobCard_JobCard__body__position__CyaGY\")\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e8f442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 타이틀 제목 리스트로 형변환 하고 몇개 데이터 있는지 길이 확인\n",
    "title_list = [item.text for item in title]\n",
    "title_list, len(title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c6519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회사 이름 확인\n",
    "company = soup.find_all(\"span\", class_ = \"Typography_Typography__root__RdAI1 Typography_Typography__label2__svmAA Typography_Typography__weightMedium__GXnOM JobCard_JobCard__body__company__AUj_B\")\n",
    "company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b5e537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포문으로 회사 이름 리스트로 생성, 요소 갯수 확인\n",
    "company_list = [item.text for item in company]\n",
    "company_list, len(company_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48de784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회사 이름, 공고문 데이터 프레임 생성\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6665bbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(company_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc842ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {df.columns[0] : '회사명'}, inplace= True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b058a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['제목'] = (title_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b13b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e2147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = \"https://www.wanted.co.kr/\"\n",
    "url_sub = \"wdlist/518/1634?country=kr&job_sort=job.recommend_order&years=-1&selected=1634&locations=seoul.gangnam-gu&locations=seoul.guro-gu&locations=seoul.geumcheon-gu&locations=seoul.seocho-gu&locations=seoul.songpa-gu\"\n",
    "url = url_base + url_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b5d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_list = soup.find_all('div', 'JobCard_JobCard__Tb7pI')\n",
    "soup_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3781861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for문으로  url 데이터 리스트로 추출\n",
    "url_add = []\n",
    "for item in soup_list:\n",
    "    url_add.append(urljoin(url_base, item.find('a')['href']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda276de",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39efe694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 데이터 프레임에 url 컬럼 추가\n",
    "df['URL'] = url_add\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502681db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "570aae92",
   "metadata": {},
   "source": [
    "페이지 내부 파싱 및 크롤링(비활성화)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff4efa",
   "metadata": {},
   "source": [
    "from urllib.request import Request, urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040a8a67",
   "metadata": {},
   "source": [
    "req =Request(df[\"URL\"][0], headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.199 Safari/537.36'})\n",
    "html = urlopen(req)\n",
    "soup_tmp = BeautifulSoup(html, 'html.parser')\n",
    "soup_tmp.find('div', class_ = 'JobHeader_JobHeader__Tools__80TiC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7396a659",
   "metadata": {},
   "source": [
    "driver.find_element(By.XPATH, '//*[@id=\"__next\"]/div[3]/div[2]/ul/li[1]/div/a/div[2]/p').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cd1255",
   "metadata": {},
   "source": [
    "element_detail = driver.find_element(By.XPATH, '//*[@id=\"__next\"]/main/div[1]/div/section/section/article[1]/div/button/span[2]').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9e4df9",
   "metadata": {},
   "source": [
    "element = driver.find_element(By.XPATH, '//*[@id=\"__next\"]/main/div[1]/div/section/header/div').text\n",
    "element.split('∙')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6e5cb8",
   "metadata": {},
   "source": [
    "driver.back()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050779ed",
   "metadata": {},
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba52bbe6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06f7e243",
   "metadata": {},
   "source": [
    "데이터프레임 내 웹페이지 주소 기반 크롤링 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71675b50",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 1. 저속 기본 크롤링 셀레니움(240825)\n",
    "\n",
    "\n",
    "# 크롬 드라이버 설정\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# 크롤링 데이터 저장을 위한 리스트\n",
    "title_list = []         # 제목 리스트\n",
    "company_list = []       # 회사 이름 리스트\n",
    "career_list = []        # 경력 리스트\n",
    "mainjob_list = []       # 주요업무 리스트\n",
    "qual_list = []          # 자격요건 리스트\n",
    "prefer_list = []        # 우대사항 리스트\n",
    "welfare_list = []       # 혜택 및 복지 리스트\n",
    "deadline_list = []      # 마감일 리스트\n",
    "location_list = []      # 근무지 리스트\n",
    "\n",
    "# 데이터프레임의 각 행에 대해 반복\n",
    "for idx, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    try:\n",
    "        # URL 접속\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # 페이지가 로드될 시간을 충분히 기다림\n",
    "\n",
    "        try:\n",
    "            # 페이지 스크롤을 조금씩 내리면서 요소를 찾음\n",
    "            while True:\n",
    "                driver.execute_script(\"window.scrollBy(0, 500);\")  # 500픽셀씩 스크롤\n",
    "                time.sleep(1)  # 스크롤 후 잠시 대기\n",
    "\n",
    "                try:\n",
    "                    # 상세보기 버튼 찾기\n",
    "                    detail_button_xpath = WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, f'//*[@id=\"__next\"]/main/div[1]/div/section/section/article[1]/div/button/span[1]'))\n",
    "                    )\n",
    "                    driver.execute_script(\"arguments[0].click();\", detail_button_xpath)\n",
    "                    break  # 클릭이 성공하면 루프를 종료\n",
    "                except (TimeoutException, NoSuchElementException):\n",
    "                    pass  # 아직 버튼이 보이지 않으면 다시 시도\n",
    "        except TimeoutException:\n",
    "            print(\"상세보기 버튼을 찾을 수 없습니다.\")\n",
    "            \n",
    "        time.sleep(3)\n",
    "        \n",
    "        # html 코드 파싱\n",
    "        soup_tmp = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # 페이지 제목 가져오기\n",
    "        try:        \n",
    "            title = driver.find_element(By.XPATH, '//*[@id=\"__next\"]/main/div[1]/div/section/header/h1').text\n",
    "        except NoSuchElementException:\n",
    "            title = \"제목 없음\"\n",
    "        time.sleep(2)\n",
    "\n",
    "        # 회사명 가져오기\n",
    "        try:        \n",
    "            company = driver.find_element(By.XPATH, '//*[@id=\"__next\"]/main/div[1]/div/section/header/div/div[1]/a').text\n",
    "        except NoSuchElementException:\n",
    "            company = \"회사명 없음\"\n",
    "        time.sleep(2)\n",
    "\n",
    "        # 경력 가져오기\n",
    "        try:        \n",
    "            career = driver.find_element(By.XPATH, '//*[@id=\"__next\"]/main/div[1]/div/section/header/div/div[1]/span[4]').text\n",
    "        except NoSuchElementException:\n",
    "            career = \"경력 없음\"\n",
    "        time.sleep(2)\n",
    "\n",
    "        # 주요업무 가져오기\n",
    "        try:\n",
    "            mainjob = driver.find_element(By.XPATH, '//*[@id=\"__next\"]/main/div[1]/div/section/section/article[1]/div/div[1]').text\n",
    "        except NoSuchElementException:\n",
    "            mainjob = \"주요 업무 없음\"\n",
    "        time.sleep(2)    \n",
    "\n",
    "        # 자격요건 가져오기\n",
    "        try:\n",
    "            qual = driver.find_element(By.XPATH, '//*[@id=\"__next\"]/main/div[1]/div/section/section/article[1]/div/div[2]').text\n",
    "        except NoSuchElementException:\n",
    "            qual = \"자격 요건 없음\"\n",
    "        time.sleep(2)    \n",
    "\n",
    "        # 우대사항 가져오기\n",
    "        try:\n",
    "            prefer = driver.find_element(By.XPATH, '//*[@id=\"__next\"]/main/div[1]/div/section/section/article[1]/div/div[3]').text\n",
    "        except NoSuchElementException:\n",
    "            prefer = \"우대 사항 없음\"\n",
    "        time.sleep(2)\n",
    "\n",
    "        # 복지 가져오기\n",
    "        try:\n",
    "            welfare = driver.find_element(By.XPATH, '//*[@id=\"__next\"]/main/div[1]/div/section/section/article[1]/div/div[4]').text\n",
    "        except NoSuchElementException:\n",
    "            welfare = \"복지 정보 없음\"\n",
    "        time.sleep(2)\n",
    "\n",
    "        # 마감일 가져오기\n",
    "        try:        \n",
    "            deadline = driver.find_element(By.XPATH, '//*[@id=\"__next\"]/main/div[1]/div/section/section/article[4]/span').text\n",
    "        except NoSuchElementException:\n",
    "            deadline = \"마감 정보 없음\"\n",
    "        time.sleep(2)\n",
    "\n",
    "        # 근무지 가져오기\n",
    "        try:        \n",
    "            location = location = soup_tmp.find('span', class_='Typography_Typography__root__RdAI1 Typography_Typography__body2__5Mmhi Typography_Typography__weightMedium__GXnOM').get_text()\n",
    "        except NoSuchElementException:\n",
    "            location = \"근무지 정보 없음\"\n",
    "\n",
    "        # 해당 정보 빈 리스트에 저장\n",
    "        title_list.append(title)\n",
    "        company_list.append(company)\n",
    "        career_list.append(career)\n",
    "        mainjob_list.append(mainjob)\n",
    "        qual_list.append(qual)\n",
    "        prefer_list.append(prefer)\n",
    "        welfare_list.append(welfare)\n",
    "        deadline_list.append(deadline)\n",
    "        location_list.append(location)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"페이지를 로드할 수 없거나, 예상치 못한 오류 발생: {e}\")\n",
    "        # 예외 발생 시 각 리스트에 예외 처리 메시지 추가\n",
    "        title_list.append(\"페이지 만료\")\n",
    "        company_list.append(\"페이지 만료\")\n",
    "        career_list.append(\"페이지 만료\")\n",
    "        mainjob_list.append(\"페이지 만료\")\n",
    "        qual_list.append(\"페이지 만료\")\n",
    "        prefer_list.append(\"페이지 만료\")\n",
    "        welfare_list.append(\"페이지 만료\")\n",
    "        deadline_list.append(\"페이지 만료\")\n",
    "        location_list.append(\"페이지 만료\")\n",
    "        \n",
    "    # 진행도 확인하기 위해 idx 출력\n",
    "    print(f\"Processed row: {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa67fb1",
   "metadata": {},
   "source": [
    "## 2. 고속화 크롤링 뷰티풀 숲(240826)\n",
    "\n",
    "\n",
    "# 크롬 드라이버 설정\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# 크롤링 데이터 저장을 위한 리스트\n",
    "title_list = []         # 제목 리스트\n",
    "company_list = []       # 회사 이름 리스트\n",
    "career_list = []        # 경력 리스트\n",
    "mainjob_list = []       # 주요업무 리스트\n",
    "qual_list = []          # 자격요건 리스트\n",
    "prefer_list = []        # 우대사항 리스트\n",
    "welfare_list = []       # 혜택 및 복지 리스트\n",
    "deadline_list = []      # 마감일 리스트\n",
    "location_list = []      # 근무지 리스트\n",
    "\n",
    "# 데이터프레임의 각 행에 대해 반복\n",
    "for idx, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    try:\n",
    "        # URL 접속\n",
    "        driver.get(url)\n",
    "        \n",
    "        # 페이지 로드 완료 대기\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"__next\"]/main/div[1]/div/section/header/h1'))\n",
    "        )\n",
    "        \n",
    "        # 상세보기 버튼 찾기\n",
    "        try:\n",
    "            while True:\n",
    "                # 스크롤을 통해 상세보기 버튼을 찾음\n",
    "                driver.execute_script(\"window.scrollBy(0, 500);\")  # 500픽셀씩 스크롤\n",
    "                try:\n",
    "                    detail_button = WebDriverWait(driver, 3).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '//*[@id=\"__next\"]/main/div[1]/div/section/section/article[1]/div/button/span[1]'))\n",
    "                    )\n",
    "                    driver.execute_script(\"arguments[0].click();\", detail_button)\n",
    "                    break  # 클릭이 성공하면 루프를 종료\n",
    "                except (TimeoutException, NoSuchElementException):\n",
    "                    pass  # 아직 버튼이 보이지 않으면 다시 시도\n",
    "        except TimeoutException:\n",
    "            print(\"상세보기 버튼을 찾을 수 없습니다.\")\n",
    "        \n",
    "        # 대기 시간 설정\n",
    "        time.sleep(random.uniform(1,3))\n",
    "        \n",
    "        # 페이지 로드 후 소스 파싱\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # 페이지 제목 가져오기\n",
    "        title = soup.select_one('h1').text if soup.select_one('h1') else \"제목 없음\"\n",
    "\n",
    "        # 회사명 가져오기\n",
    "        company = soup.select_one('header div a').text if soup.select_one('header div a') else \"회사명 없음\"\n",
    "\n",
    "        # 경력 가져오기\n",
    "        career = soup.select_one('header div span:nth-of-type(4)').text if soup.select_one('header div span:nth-of-type(4)') else \"경력 없음\"\n",
    "\n",
    "        # 주요업무 가져오기\n",
    "        mainjob = soup.select_one('article div div:nth-of-type(1)').text if soup.select_one('article div div:nth-of-type(1)') else \"주요 업무 없음\"\n",
    "\n",
    "        # 자격요건 가져오기\n",
    "        qual = soup.select_one('article div div:nth-of-type(2)').text if soup.select_one('article div div:nth-of-type(2)') else \"자격 요건 없음\"\n",
    "\n",
    "        # 우대사항 가져오기\n",
    "        prefer = soup.select_one('article div div:nth-of-type(3)').text if soup.select_one('article div div:nth-of-type(3)') else \"우대 사항 없음\"\n",
    "\n",
    "        # 복지 가져오기\n",
    "        welfare = soup.select_one('article div div:nth-of-type(4)').text if soup.select_one('article div div:nth-of-type(4)') else \"복지 정보 없음\"\n",
    "\n",
    "        # 마감일 가져오기\n",
    "        deadline = soup.select_one('article:nth-of-type(4) span').text if soup.select_one('article:nth-of-type(4) span') else \"마감 정보 없음\"\n",
    "\n",
    "        # 근무지 가져오기\n",
    "        location = soup.find('span', class_='Typography_Typography__root__RdAI1 Typography_Typography__body2__5Mmhi Typography_Typography__weightMedium__GXnOM').text if soup.find('span', class_='Typography_Typography__root__RdAI1 Typography_Typography__body2__5Mmhi Typography_Typography__weightMedium__GXnOM') else \"근무지 정보 없음\"\n",
    "\n",
    "        # 해당 정보 빈 리스트에 저장\n",
    "        title_list.append(title)\n",
    "        company_list.append(company)\n",
    "        career_list.append(career)\n",
    "        mainjob_list.append(mainjob)\n",
    "        qual_list.append(qual)\n",
    "        prefer_list.append(prefer)\n",
    "        welfare_list.append(welfare)\n",
    "        deadline_list.append(deadline)\n",
    "        location_list.append(location)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"페이지를 로드할 수 없거나, 예상치 못한 오류 발생: {e}\")\n",
    "        # 예외 발생 시 각 리스트에 예외 처리 메시지 추가\n",
    "        title_list.append(\"페이지 만료\")\n",
    "        company_list.append(\"페이지 만료\")\n",
    "        career_list.append(\"페이지 만료\")\n",
    "        mainjob_list.append(\"페이지 만료\")\n",
    "        qual_list.append(\"페이지 만료\")\n",
    "        prefer_list.append(\"페이지 만료\")\n",
    "        welfare_list.append(\"페이지 만료\")\n",
    "        deadline_list.append(\"페이지 만료\")\n",
    "        location_list.append(\"페이지 만료\")\n",
    "        \n",
    "    # 진행도 확인하기 위해 idx 출력\n",
    "    print(f\"Processed row: {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec086482",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 3. 고속화 크롤링 뷰티풀숲, 스텔스 기능 (240827) 81.56초\n",
    "\n",
    "\n",
    "# 크롬 드라이버 설정\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# 스텔스 기능 적용\n",
    "stealth(driver,\n",
    "        languages=[\"en-US\", \"en\"],\n",
    "        vendor=\"Google Inc.\",\n",
    "        platform=\"Win32\",\n",
    "        webgl_vendor=\"Intel Inc.\",\n",
    "        renderer=\"Intel Iris OpenGL Engine\",\n",
    "        fix_hairline=True,\n",
    ")\n",
    "\n",
    "# 크롤링 데이터 저장을 위한 리스트\n",
    "title_list = []         # 제목 리스트\n",
    "company_list = []       # 회사 이름 리스트\n",
    "career_list = []        # 경력 리스트\n",
    "mainjob_list = []       # 주요업무 리스트\n",
    "qual_list = []          # 자격요건 리스트\n",
    "prefer_list = []        # 우대사항 리스트\n",
    "welfare_list = []       # 혜택 및 복지 리스트\n",
    "deadline_list = []      # 마감일 리스트\n",
    "location_list = []      # 근무지 리스트\n",
    "\n",
    "# 크롤링 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "# 데이터프레임의 각 행에 대해 반복\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Crawling Progress\"):\n",
    "    url = row['URL']\n",
    "    try:\n",
    "        # URL 접속\n",
    "        driver.get(url)\n",
    "        \n",
    "        # 페이지 로드 완료 대기\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"__next\"]/main/div[1]/div/section/header/h1'))\n",
    "        )\n",
    "        \n",
    "        # 상세보기 버튼 찾기\n",
    "        try:\n",
    "            while True:\n",
    "                # 스크롤을 통해 상세보기 버튼을 찾음\n",
    "                driver.execute_script(\"window.scrollBy(0, 500);\")  # 500픽셀씩 스크롤\n",
    "                try:\n",
    "                    detail_button = WebDriverWait(driver, 3).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '//*[@id=\"__next\"]/main/div[1]/div/section/section/article[1]/div/button/span[1]'))\n",
    "                    )\n",
    "                    driver.execute_script(\"arguments[0].click();\", detail_button)\n",
    "                    break  # 클릭이 성공하면 루프를 종료\n",
    "                except (TimeoutException, NoSuchElementException):\n",
    "                    pass  # 아직 버튼이 보이지 않으면 다시 시도\n",
    "        except TimeoutException:\n",
    "            print(\"상세보기 버튼을 찾을 수 없습니다.\")\n",
    "        \n",
    "        # 대기 시간 설정\n",
    "        time.sleep(random.uniform(1,3))\n",
    "        \n",
    "        # 페이지 로드 후 소스 파싱\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # 페이지 제목 가져오기\n",
    "        title = soup.select_one('h1').text if soup.select_one('h1') else \"제목 없음\"\n",
    "\n",
    "        # 회사명 가져오기\n",
    "        company = soup.select_one('header div a').text if soup.select_one('header div a') else \"회사명 없음\"\n",
    "\n",
    "        # 경력 가져오기\n",
    "        career = soup.select_one('header div span:nth-of-type(4)').text if soup.select_one('header div span:nth-of-type(4)') else \"경력 없음\"\n",
    "\n",
    "        # 주요업무 가져오기\n",
    "        mainjob = soup.select_one('article div div:nth-of-type(1)').text if soup.select_one('article div div:nth-of-type(1)') else \"주요 업무 없음\"\n",
    "\n",
    "        # 자격요건 가져오기\n",
    "        qual = soup.select_one('article div div:nth-of-type(2)').text if soup.select_one('article div div:nth-of-type(2)') else \"자격 요건 없음\"\n",
    "\n",
    "        # 우대사항 가져오기\n",
    "        prefer = soup.select_one('article div div:nth-of-type(3)').text if soup.select_one('article div div:nth-of-type(3)') else \"우대 사항 없음\"\n",
    "\n",
    "        # 복지 가져오기\n",
    "        welfare = soup.select_one('article div div:nth-of-type(4)').text if soup.select_one('article div div:nth-of-type(4)') else \"복지 정보 없음\"\n",
    "\n",
    "        # 마감일 가져오기\n",
    "        deadline = soup.select_one('article:nth-of-type(4) span').text if soup.select_one('article:nth-of-type(4) span') else \"마감 정보 없음\"\n",
    "\n",
    "        # 근무지 가져오기\n",
    "        location = soup.find('span', class_='Typography_Typography__root__RdAI1 Typography_Typography__body2__5Mmhi Typography_Typography__weightMedium__GXnOM').text if soup.find('span', class_='Typography_Typography__root__RdAI1 Typography_Typography__body2__5Mmhi Typography_Typography__weightMedium__GXnOM') else \"근무지 정보 없음\"\n",
    "\n",
    "        # 해당 정보 빈 리스트에 저장\n",
    "        title_list.append(title)\n",
    "        company_list.append(company)\n",
    "        career_list.append(career)\n",
    "        mainjob_list.append(mainjob)\n",
    "        qual_list.append(qual)\n",
    "        prefer_list.append(prefer)\n",
    "        welfare_list.append(welfare)\n",
    "        deadline_list.append(deadline)\n",
    "        location_list.append(location)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"페이지를 로드할 수 없거나, 예상치 못한 오류 발생: {e}\")\n",
    "        # 예외 발생 시 각 리스트에 예외 처리 메시지 추가\n",
    "        title_list.append(\"페이지 만료\")\n",
    "        company_list.append(\"페이지 만료\")\n",
    "        career_list.append(\"페이지 만료\")\n",
    "        mainjob_list.append(\"페이지 만료\")\n",
    "        qual_list.append(\"페이지 만료\")\n",
    "        prefer_list.append(\"페이지 만료\")\n",
    "        welfare_list.append(\"페이지 만료\")\n",
    "        deadline_list.append(\"페이지 만료\")\n",
    "        location_list.append(\"페이지 만료\")\n",
    "        \n",
    "    # 진행도 확인하기 위해 idx 출력\n",
    "    print(f\"Processed row: {idx}\")\n",
    "    \n",
    "# 크롤링 종료 시간 기록\n",
    "end_time = time.time()\n",
    "\n",
    "# 크롤링 총 시간 계산 및 출력\n",
    "total_time = end_time - start_time\n",
    "print(f\"크롤링 작업 완료. 총 소요 시간: {total_time:.2f}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debf0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. 고속화 크롤링 뷰티풀숲, 스텔스, 헤드리스, 이미지로드 비활성화, user-Agent변경 기능 적용 (240827) 79.27초 소요\n",
    "## 완성 1 형\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium_stealth import stealth\n",
    "\n",
    "# 크롬 옵션 설정\n",
    "chrome_options = Options()\n",
    "\n",
    "# 헤드리스 모드\n",
    "chrome_options.add_argument(\"--headless\")  \n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# 이미지 로드 비활성화\n",
    "chrome_options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "\n",
    "# User-Agent 변경\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\")\n",
    "\n",
    "# 크롬 드라이버 설정\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# 스텔스 기능 적용\n",
    "stealth(driver,\n",
    "        languages=[\"en-US\", \"en\"],\n",
    "        vendor=\"Google Inc.\",\n",
    "        platform=\"Win32\",\n",
    "        webgl_vendor=\"Intel Inc.\",\n",
    "        renderer=\"Intel Iris OpenGL Engine\",\n",
    "        fix_hairline=True,\n",
    ")\n",
    "\n",
    "# 크롤링 데이터 저장을 위한 리스트\n",
    "title_list = []         # 제목 리스트\n",
    "company_list = []       # 회사 이름 리스트\n",
    "career_list = []        # 경력 리스트\n",
    "mainjob_list = []       # 주요업무 리스트\n",
    "qual_list = []          # 자격요건 리스트\n",
    "prefer_list = []        # 우대사항 리스트\n",
    "welfare_list = []       # 혜택 및 복지 리스트\n",
    "skill_list = []         # 스킬 리스트\n",
    "tag_list = []           # 태그 리스트\n",
    "deadline_list = []      # 마감일 리스트\n",
    "location_list = []      # 근무지 리스트\n",
    "\n",
    "# 크롤링 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "# 데이터프레임의 각 행에 대해 반복\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Crawling Progress\", ncols=100, position=0, leave=True):\n",
    "    url = row['URL']\n",
    "    try:\n",
    "        # URL 접속\n",
    "        driver.get(url)\n",
    "        \n",
    "        # 페이지 로드 완료 대기\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"__next\"]/main/div[1]/div/section/header/h1'))\n",
    "        )\n",
    "        \n",
    "        # 상세보기 버튼 찾기\n",
    "        try:\n",
    "            while True:\n",
    "                # 스크롤을 통해 상세보기 버튼을 찾음\n",
    "                driver.execute_script(\"window.scrollBy(0, 500);\")  # 500픽셀씩 스크롤\n",
    "                try:\n",
    "                    detail_button = WebDriverWait(driver, 3).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '//*[@id=\"__next\"]/main/div[1]/div/section/section/article[1]/div/button/span[1]'))\n",
    "                    )\n",
    "                    driver.execute_script(\"arguments[0].click();\", detail_button)\n",
    "                    break  # 클릭이 성공하면 루프를 종료\n",
    "                except (TimeoutException, NoSuchElementException):\n",
    "                    pass  # 아직 버튼이 보이지 않으면 다시 시도\n",
    "        except TimeoutException:\n",
    "            print(\"상세보기 버튼을 찾을 수 없습니다.\")\n",
    "        \n",
    "        # 대기 시간 설정\n",
    "        time.sleep(random.uniform(1,3))\n",
    "        \n",
    "        # 페이지 로드 후 소스 파싱\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # 페이지 제목 가져오기\n",
    "        title = soup.select_one('h1').text if soup.select_one('h1') else \"제목 없음\"\n",
    "\n",
    "        # 회사명 가져오기\n",
    "        company = soup.select_one('header div a').text if soup.select_one('header div a') else \"회사명 없음\"\n",
    "\n",
    "        # 경력 가져오기\n",
    "        career = soup.select_one('header div span:nth-of-type(4)').text if soup.select_one('header div span:nth-of-type(4)') else \"경력 없음\"\n",
    "\n",
    "        # 주요업무 가져오기\n",
    "        mainjob = soup.select_one('article div div:nth-of-type(1)').text if soup.select_one('article div div:nth-of-type(1)') else \"주요 업무 없음\"\n",
    "\n",
    "        # 자격요건 가져오기\n",
    "        qual = soup.select_one('article div div:nth-of-type(2)').text if soup.select_one('article div div:nth-of-type(2)') else \"자격 요건 없음\"\n",
    "\n",
    "        # 우대사항 가져오기\n",
    "        prefer = soup.select_one('article div div:nth-of-type(3)').text if soup.select_one('article div div:nth-of-type(3)') else \"우대 사항 없음\"\n",
    "\n",
    "        # 복지 가져오기\n",
    "        welfare = soup.select_one('article div div:nth-of-type(4)').text if soup.select_one('article div div:nth-of-type(4)') else \"복지 정보 없음\"\n",
    "\n",
    "        # 스킬 가져오기\n",
    "        skill = soup.select_one('.JobSkillTags_JobSkillTags__list__01GRk').text if soup.select_one('.JobSkillTags_JobSkillTags__list__01GRk') else \"스킬 없음\"\n",
    "\n",
    "        # 태그 가져오기\n",
    "        tag = soup.select_one('.CompanyTags_CompanyTags__list__WjcTV').text if soup.select_one('.CompanyTags_CompanyTags__list__WjcTV') else \"태그 없음\"\n",
    "        \n",
    "        # 마감일 가져오기\n",
    "        deadline = soup.select_one('.JobDueTime_JobDueTime__3yzxa').text if soup.select_one('.JobDueTime_JobDueTime__3yzxa') else \"마감 정보 없음\"\n",
    "\n",
    "        # 근무지 가져오기\n",
    "        location = soup.find('span', class_='Typography_Typography__root__RdAI1 Typography_Typography__body2__5Mmhi Typography_Typography__weightMedium__GXnOM').text if soup.find('span', class_='Typography_Typography__root__RdAI1 Typography_Typography__body2__5Mmhi Typography_Typography__weightMedium__GXnOM') else \"근무지 정보 없음\"\n",
    "\n",
    "        # 해당 정보 빈 리스트에 저장\n",
    "        title_list.append(title)\n",
    "        company_list.append(company)\n",
    "        career_list.append(career)\n",
    "        mainjob_list.append(mainjob)\n",
    "        qual_list.append(qual)\n",
    "        prefer_list.append(prefer)\n",
    "        welfare_list.append(welfare)\n",
    "        skill_list.append(skill)\n",
    "        tag_list.append(tag)\n",
    "        deadline_list.append(deadline)\n",
    "        location_list.append(location)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"페이지를 로드할 수 없거나, 예상치 못한 오류 발생: {e}\")\n",
    "        # 예외 발생 시 각 리스트에 예외 처리 메시지 추가\n",
    "        title_list.append(\"페이지 만료\")\n",
    "        company_list.append(\"페이지 만료\")\n",
    "        career_list.append(\"페이지 만료\")\n",
    "        mainjob_list.append(\"페이지 만료\")\n",
    "        qual_list.append(\"페이지 만료\")\n",
    "        prefer_list.append(\"페이지 만료\")\n",
    "        welfare_list.append(\"페이지 만료\")\n",
    "        skill_list.append(\"페이지 만료\")\n",
    "        tag_list.append(\"페이지 만료\")\n",
    "        deadline_list.append(\"페이지 만료\")\n",
    "        location_list.append(\"페이지 만료\")\n",
    "        \n",
    "# 드라이버 종료\n",
    "driver.quit()\n",
    "    \n",
    "# 크롤링 종료 시간 기록\n",
    "end_time = time.time()\n",
    "\n",
    "# 크롤링 총 시간 계산 및 출력\n",
    "total_time = end_time - start_time\n",
    "print(f\"크롤링 작업 완료. 총 소요 시간: {total_time:.2f}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc8fd23",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36c50e5c",
   "metadata": {},
   "source": [
    "## 5. 고속화 크롤링 뷰티풀숲, 스텔스, 헤드리스, 이미지로드 비활성화, user-Agent변경, 병렬작업 기능 적용 (240827) 467초 소요(오류 발생)\n",
    "## 완성 2형\n",
    "\n",
    "# 크롬 옵션 설정\n",
    "chrome_options = Options()\n",
    "\n",
    "# 헤드리스 모드 및 기타 옵션 설정\n",
    "chrome_options.add_argument(\"--headless\")  \n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\")\n",
    "\n",
    "# 크롤링 데이터 저장을 위한 리스트\n",
    "title_list = []         # 제목 리스트\n",
    "company_list = []       # 회사 이름 리스트\n",
    "career_list = []        # 경력 리스트\n",
    "mainjob_list = []       # 주요업무 리스트\n",
    "qual_list = []          # 자격요건 리스트\n",
    "prefer_list = []        # 우대사항 리스트\n",
    "welfare_list = []       # 혜택 및 복지 리스트\n",
    "skill_list = []         # 스킬 리스트\n",
    "tag_list = []           # 태그 리스트\n",
    "deadline_list = []      # 마감일 리스트\n",
    "location_list = []      # 근무지 리스트\n",
    "\n",
    "# URL 크롤링 함수 정의\n",
    "def crawl_data(url):\n",
    "    try:\n",
    "        # 크롬 드라이버 설정\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        \n",
    "        # 스텔스 기능 적용\n",
    "        stealth(driver,\n",
    "            languages=[\"en-US\", \"en\"],\n",
    "            vendor=\"Google Inc.\",\n",
    "            platform=\"Win32\",\n",
    "            webgl_vendor=\"Intel Inc.\",\n",
    "            renderer=\"Intel Iris OpenGL Engine\",\n",
    "            fix_hairline=True,\n",
    "        )\n",
    "\n",
    "        # URL 접속\n",
    "        driver.get(url)\n",
    "\n",
    "        # 페이지 로드 완료 대기\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"__next\"]/main/div[1]/div/section/header/h1'))\n",
    "        )\n",
    "\n",
    "        # 상세보기 버튼 찾기 및 클릭\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollBy(0, 500);\")\n",
    "            try:\n",
    "                detail_button = WebDriverWait(driver, 3).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '//*[@id=\"__next\"]/main/div[1]/div/section/section/article[1]/div/button/span[1]'))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].click();\", detail_button)\n",
    "                break\n",
    "            except (TimeoutException, NoSuchElementException):\n",
    "                pass\n",
    "\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "        # 페이지 로드 후 소스 파싱\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # 크롤링할 데이터 추출\n",
    "        title = soup.select_one('h1').text if soup.select_one('h1') else \"제목 없음\"\n",
    "        company = soup.select_one('header div a').text if soup.select_one('header div a') else \"회사명 없음\"\n",
    "        career = soup.select_one('header div span:nth-of-type(4)').text if soup.select_one('header div span:nth-of-type(4)') else \"경력 없음\"\n",
    "        mainjob = soup.select_one('article div div:nth-of-type(1)').text if soup.select_one('article div div:nth-of-type(1)') else \"주요 업무 없음\"\n",
    "        qual = soup.select_one('article div div:nth-of-type(2)').text if soup.select_one('article div div:nth-of-type(2)') else \"자격 요건 없음\"\n",
    "        prefer = soup.select_one('article div div:nth-of-type(3)').text if soup.select_one('article div div:nth-of-type(3)') else \"우대 사항 없음\"\n",
    "        welfare = soup.select_one('article div div:nth-of-type(4)').text if soup.select_one('article div div:nth-of-type(4)') else \"복지 정보 없음\"\n",
    "        skill = soup.select_one('.JobSkillTags_JobSkillTags__list__01GRk').text if soup.select_one('.JobSkillTags_JobSkillTags__list__01GRk') else \"스킬 없음\"\n",
    "        tag = soup.select_one('.CompanyTags_CompanyTags__list__WjcTV').text if soup.select_one('.CompanyTags_CompanyTags__list__WjcTV') else \"태그 없음\"\n",
    "        deadline = soup.select_one('.JobDueTime_JobDueTime__3yzxa').text if soup.select_one('.JobDueTime_JobDueTime__3yzxa') else \"마감 정보 없음\"\n",
    "        location = soup.find('span', class_='Typography_Typography__root__RdAI1 Typography_Typography__body2__5Mmhi Typography_Typography__weightMedium__GXnOM').text if soup.find('span', class_='Typography_Typography__root__RdAI1 Typography_Typography__body2__5Mmhi Typography_Typography__weightMedium__GXnOM') else \"근무지 정보 없음\"\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"company\": company,\n",
    "            \"career\": career,\n",
    "            \"mainjob\": mainjob,\n",
    "            \"qual\": qual,\n",
    "            \"prefer\": prefer,\n",
    "            \"welfare\": welfare,\n",
    "            \"skill\": skill,\n",
    "            \"tag\": tag,\n",
    "            \"deadline\": deadline,\n",
    "            \"location\": location\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"페이지를 로드할 수 없거나, 예상치 못한 오류 발생: {e}\")\n",
    "        return {\n",
    "            \"title\": \"페이지 만료\",\n",
    "            \"company\": \"페이지 만료\",\n",
    "            \"career\": \"페이지 만료\",\n",
    "            \"mainjob\": \"페이지 만료\",\n",
    "            \"qual\": \"페이지 만료\",\n",
    "            \"prefer\": \"페이지 만료\",\n",
    "            \"welfare\": \"페이지 만료\",\n",
    "            \"skill\": \"페이지 만료\",\n",
    "            \"tag\": \"페이지 만료\",\n",
    "            \"deadline\": \"페이지 만료\",\n",
    "            \"location\": \"페이지 만료\"\n",
    "        }\n",
    "\n",
    "# 크롤링 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "# 병렬 크롤링 실행\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:  # 2개의 스레드 사용\n",
    "    futures = [executor.submit(crawl_data, row['URL']) for idx, row in df.iterrows()]\n",
    "    \n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Crawling Progress\"):\n",
    "        result = future.result()\n",
    "        title_list.append(result[\"title\"])\n",
    "        company_list.append(result[\"company\"])\n",
    "        career_list.append(result[\"career\"])\n",
    "        mainjob_list.append(result[\"mainjob\"])\n",
    "        qual_list.append(result[\"qual\"])\n",
    "        prefer_list.append(result[\"prefer\"])\n",
    "        welfare_list.append(result[\"welfare\"])\n",
    "        skill_list.append(result[\"skill\"])\n",
    "        tag_list.append(result[\"tag\"])\n",
    "        deadline_list.append(result[\"deadline\"])\n",
    "        location_list.append(result[\"location\"])\n",
    "\n",
    "# 크롤링 종료 시간 기록\n",
    "end_time = time.time()\n",
    "\n",
    "# 크롤링 총 시간 계산 및 출력\n",
    "total_time = end_time - start_time\n",
    "print(f\"크롤링 작업 완료. 총 소요 시간: {total_time:.2f}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334e8b54",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "385e9d18",
   "metadata": {},
   "source": [
    "데이터 프레임 정리 후 파일 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85efa930",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pymysql 설치(DB적재)\n",
    "!pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1e01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임 형성 및 적재 목적 라이브러리 import\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf74266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임에 데이터 추가\n",
    "df['Title'] = title_list\n",
    "df['Company'] = company_list\n",
    "df['Career'] = career_list\n",
    "df['Mainjob'] = mainjob_list\n",
    "df['Qualified'] = qual_list\n",
    "df['Preference'] = prefer_list\n",
    "df['Welfare'] = welfare_list\n",
    "df['Skill'] = skill_list\n",
    "df['Tag'] = tag_list\n",
    "df['Deadline'] = deadline_list\n",
    "df['Location'] = location_list\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57df65dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, ['Company', 'Title', 'Career', 'Mainjob', 'Qualified', 'Preference', 'Welfare', 'Skill', 'Tag', 'Deadline', 'Location', 'URL']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5b3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 csv파일로 저장\n",
    "df.to_csv('data/Wanted_crawling_data.csv', sep=\",\", encoding='cp949', errors='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01289477",
   "metadata": {},
   "source": [
    "# 데이터프레임 형성 및 구를 기준으로 색인하기 위한 인덱스 설정(데이터 전처리 전 기준점에 맞춰 컬럼 설정)\n",
    "data = {'Title':title_list, 'Company':company_list, 'Career':career_list, 'Mainjob':mainjob_list, 'Qualified':qual_list, '':edu_list, 'InfoDetails':personality_list, 'Tag':em_type_list, 'D_day':pay_list}\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index('Address', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba53ab04",
   "metadata": {},
   "source": [
    "전처리 작업 후 데이터 적재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee91b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주소를 구만 남김\n",
    "address_list = [item[-3:] for item in address_list]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
